const axios = require('axios')
const { PrismaClient } = require('@prisma/client')

const prisma = new PrismaClient()

async function testFirecrawlV1Api() {
    try {
        console.log('üî• Testing Updated Firecrawl V1 API Implementation')
        console.log('='.repeat(60))

        // Get API key from database
        const config = await prisma.cOApiConfigurations.findFirst({
            where: { isActive: true },
            orderBy: { createdAt: 'desc' }
        })

        if (!config || !config.firecrawlApiKey) {
            throw new Error('Firecrawl API key not found in database')
        }

        console.log('‚úÖ Found Firecrawl API key in database')

        // Create axios instance with V1 API settings
        const axiosInstance = axios.create({
            baseURL: 'https://api.firecrawl.dev',
            timeout: 60000,
            headers: {
                'Authorization': `Bearer ${config.firecrawlApiKey}`,
                'Content-Type': 'application/json',
            },
        })

        const testUrl = 'https://example.com'

        console.log('\n1. Testing V1 Scrape API')
        console.log('-'.repeat(40))

        try {
            const startTime = Date.now()

            // V1 API scrape payload
            const scrapePayload = {
                url: testUrl,
                formats: ['markdown'],
                onlyMainContent: true,
                excludeTags: ['script', 'style', 'nav', 'footer'],
                timeout: 30000,
                waitFor: 0,
                mobile: false,
                skipTlsVerification: false,
                removeBase64Images: true,
                blockAds: true
            }

            console.log('üì§ Making V1 scrape request...')
            console.log('üìã Payload:', JSON.stringify(scrapePayload, null, 2))

            const response = await axiosInstance.post('/v1/scrape', scrapePayload)
            const duration = Date.now() - startTime

            console.log('‚úÖ V1 Scraping successful!')
            console.log(`‚è±Ô∏è  Duration: ${duration}ms`)
            console.log(`üìä Response status: ${response.status}`)

            const data = response.data
            console.log(`üîç Response success: ${data.success}`)

            if (data.success && data.data) {
                const scrapedData = data.data
                console.log(`üìÑ Title: ${scrapedData.metadata?.title || 'No title'}`)
                console.log(`üìù Description: ${scrapedData.metadata?.description || 'No description'}`)
                console.log(`üìä Content length: ${scrapedData.content?.length || 0} characters`)
                console.log(`üìã Markdown length: ${scrapedData.markdown?.length || 0} characters`)
                console.log(`üåê Source URL: ${scrapedData.metadata?.sourceURL || testUrl}`)
                console.log(`üì± Status Code: ${scrapedData.metadata?.statusCode || 'Unknown'}`)

                if (scrapedData.markdown && scrapedData.markdown.length > 0) {
                    console.log('üìÑ Markdown preview (first 200 chars):')
                    console.log(scrapedData.markdown.substring(0, 200) + '...')
                }
            } else {
                console.error('‚ùå V1 Scraping failed:', data.error || 'Unknown error')
            }

        } catch (error) {
            console.error('‚ùå V1 Scraping failed:', error.message)
            if (error.response?.data) {
                console.error('üìÑ API Response:', JSON.stringify(error.response.data, null, 2))
            }
            if (error.response?.status) {
                console.error(`üìä HTTP Status: ${error.response.status}`)
            }
        }

        console.log('\n2. Testing V1 Structured Extraction')
        console.log('-'.repeat(40))

        try {
            const startTime = Date.now()

            // V1 API extraction payload with extract format
            const extractPayload = {
                url: testUrl,
                formats: ['extract'],
                onlyMainContent: true,
                timeout: 30000,
                extract: {
                    schema: {
                        type: "object",
                        properties: {
                            company_name: {
                                type: "string",
                                description: "The name of the company"
                            },
                            description: {
                                type: "string",
                                description: "Brief description of what the company does"
                            },
                            industry: {
                                type: "string",
                                description: "The industry or sector the company operates in"
                            }
                        },
                        required: ["company_name", "description"]
                    },
                    systemPrompt: "You are an expert at extracting company information from websites.",
                    prompt: "Extract company information from this website including company name, description, and industry."
                },
                removeBase64Images: true,
                blockAds: true
            }

            console.log('üì§ Making V1 extraction request...')
            const response = await axiosInstance.post('/v1/scrape', extractPayload)
            const duration = Date.now() - startTime

            console.log('‚úÖ V1 Data extraction successful!')
            console.log(`‚è±Ô∏è  Duration: ${duration}ms`)
            console.log(`üìä Response status: ${response.status}`)

            const data = response.data
            if (data.success && data.data?.llm_extraction) {
                console.log('üìä Extracted data:')
                console.log(JSON.stringify(data.data.llm_extraction, null, 2))
            } else {
                console.log('‚ö†Ô∏è  No extraction data returned')
                if (data.data) {
                    console.log('üìÑ Available data keys:', Object.keys(data.data))
                }
            }

        } catch (error) {
            console.error('‚ùå V1 Data extraction failed:', error.message)
            if (error.response?.data) {
                console.error('üìÑ API Response:', JSON.stringify(error.response.data, null, 2))
            }
        }

        console.log('\n3. Testing Email Domain Extraction')
        console.log('-'.repeat(40))

        // Test the new email domain extraction method
        const testEmails = [
            'john@example.com',
            'contact@firecrawl.dev',
            'admin@gmail.com', // Should be filtered out
            'user@hotmail.com', // Should be filtered out
            'info@microsoft.com',
            'invalid-email' // Should return null
        ]

        for (const email of testEmails) {
            console.log(`üìß Testing email: ${email}`)

            // Simulate the extraction logic
            if (!email || !email.includes('@')) {
                console.log(`   ‚ùå Invalid email format`)
                continue
            }

            const emailDomain = email.split('@')[1]?.toLowerCase()
            if (!emailDomain) {
                console.log(`   ‚ùå Could not extract domain`)
                continue
            }

            const freeEmailProviders = [
                'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com',
                'icloud.com', 'live.com', 'aol.com', 'protonmail.com'
            ]

            if (freeEmailProviders.includes(emailDomain)) {
                console.log(`   ‚ö†Ô∏è  Skipped: Free email provider`)
                continue
            }

            const websiteUrl = `https://www.${emailDomain}`
            console.log(`   ‚úÖ Extracted website: ${websiteUrl}`)
        }

        console.log('\n4. Testing V1 Crawl API (if credits available)')
        console.log('-'.repeat(40))

        try {
            // V1 API crawl payload
            const crawlPayload = {
                url: testUrl,
                excludePaths: [
                    "/admin/*", "/login/*", "/auth/*", "/api/*"
                ],
                includePaths: [],
                maxDepth: 2,
                ignoreSitemap: false,
                limit: 3, // Small limit for testing
                allowBackwardLinks: false,
                allowExternalLinks: false,
                scrapeOptions: {
                    formats: ['markdown'],
                    onlyMainContent: true,
                    excludeTags: ['script', 'style', 'nav', 'footer', 'header'],
                    timeout: 30000,
                    waitFor: 0,
                    mobile: false,
                    skipTlsVerification: false,
                    removeBase64Images: true,
                    blockAds: true
                }
            }

            console.log('üì§ Starting V1 crawl job...')
            const crawlResponse = await axiosInstance.post('/v1/crawl', crawlPayload)

            if (crawlResponse.data?.success) {
                const jobId = crawlResponse.data.id
                console.log(`‚úÖ Crawl job started with ID: ${jobId}`)

                // Poll for completion (simplified)
                let attempts = 0
                const maxAttempts = 10

                while (attempts < maxAttempts) {
                    await new Promise(resolve => setTimeout(resolve, 3000)) // Wait 3 seconds

                    try {
                        const statusResponse = await axiosInstance.get(`/v1/crawl/${jobId}`)
                        const status = statusResponse.data

                        console.log(`üîç Crawl status: ${status.status} (${status.current || 0}/${status.total || 0})`)

                        if (status.status === 'completed') {
                            console.log(`‚úÖ Crawl completed successfully with ${status.data?.length || 0} pages`)
                            if (status.data && status.data.length > 0) {
                                console.log('üìÑ First page preview:')
                                const firstPage = status.data[0]
                                console.log(`   Title: ${firstPage.metadata?.title || 'No title'}`)
                                console.log(`   URL: ${firstPage.metadata?.sourceURL || 'No URL'}`)
                                console.log(`   Content length: ${firstPage.markdown?.length || 0} chars`)
                            }
                            break
                        } else if (status.status === 'failed') {
                            console.error(`‚ùå Crawl failed: ${status.error || 'Unknown error'}`)
                            break
                        }
                    } catch (pollError) {
                        console.error(`‚ùå Error polling crawl status: ${pollError.message}`)
                        break
                    }

                    attempts++
                }

                if (attempts >= maxAttempts) {
                    console.log('‚è∞ Crawl polling timeout - job may still be running')
                }

            } else {
                console.error('‚ùå Failed to start crawl job:', crawlResponse.data?.error || 'Unknown error')
            }

        } catch (error) {
            console.error('‚ùå V1 Crawl test failed:', error.message)
            if (error.response?.status === 402) {
                console.log('üí≥ Payment required - crawl feature needs credits')
            } else if (error.response?.data) {
                console.error('üìÑ API Response:', JSON.stringify(error.response.data, null, 2))
            }
        }

        console.log('\n' + '='.repeat(60))
        console.log('üéâ Firecrawl V1 API testing completed!')
        console.log('')
        console.log('SUMMARY:')
        console.log('‚úÖ V1 API structure and authentication working')
        console.log('‚úÖ Updated payload formats for V1 compatibility')
        console.log('‚úÖ Email domain extraction logic implemented')
        console.log('‚úÖ Enhanced error handling for billing/rate limits')
        console.log('‚úÖ Improved content parsing and data extraction')

    } catch (error) {
        console.error('‚ùå Test failed:', error.message)
        if (error.stack) {
            console.error('Stack trace:', error.stack)
        }
    } finally {
        await prisma.$disconnect()
    }
}

testFirecrawlV1Api() 